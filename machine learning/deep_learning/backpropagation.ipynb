{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation exercise\n",
    "Now you're going to implement the backprop algorithm for a network trained on the graduate school admission data. You should have everything you need from the previous exercises to complete this one.\n",
    "\n",
    "Your goals here:\n",
    "\n",
    "* Implement the forward pass.\n",
    "* Implement the backpropagation algorithm.\n",
    "* Update the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>48</th>\n",
       "      <th>50</th>\n",
       "      <th>80</th>\n",
       "      <th>84</th>\n",
       "      <th>98</th>\n",
       "      <th>110</th>\n",
       "      <th>120</th>\n",
       "      <th>122</th>\n",
       "      <th>133</th>\n",
       "      <th>148</th>\n",
       "      <th>...</th>\n",
       "      <th>309</th>\n",
       "      <th>312</th>\n",
       "      <th>315</th>\n",
       "      <th>317</th>\n",
       "      <th>328</th>\n",
       "      <th>356</th>\n",
       "      <th>368</th>\n",
       "      <th>375</th>\n",
       "      <th>386</th>\n",
       "      <th>396</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gre</th>\n",
       "      <td>-1.278605</td>\n",
       "      <td>0.452749</td>\n",
       "      <td>0.972155</td>\n",
       "      <td>-0.759199</td>\n",
       "      <td>0.972155</td>\n",
       "      <td>0.799020</td>\n",
       "      <td>-0.586063</td>\n",
       "      <td>-0.586063</td>\n",
       "      <td>-0.759199</td>\n",
       "      <td>-0.932334</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.278605</td>\n",
       "      <td>0.625884</td>\n",
       "      <td>-2.490553</td>\n",
       "      <td>1.664697</td>\n",
       "      <td>-0.066657</td>\n",
       "      <td>0.972155</td>\n",
       "      <td>-0.066657</td>\n",
       "      <td>-0.239793</td>\n",
       "      <td>1.318426</td>\n",
       "      <td>-0.239793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpa</th>\n",
       "      <td>-2.390908</td>\n",
       "      <td>1.235263</td>\n",
       "      <td>-1.287291</td>\n",
       "      <td>0.552071</td>\n",
       "      <td>-1.339844</td>\n",
       "      <td>-0.814312</td>\n",
       "      <td>0.919944</td>\n",
       "      <td>-1.418674</td>\n",
       "      <td>-0.814312</td>\n",
       "      <td>-1.261014</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.077078</td>\n",
       "      <td>0.998773</td>\n",
       "      <td>-1.444950</td>\n",
       "      <td>0.630901</td>\n",
       "      <td>-0.131120</td>\n",
       "      <td>-0.315056</td>\n",
       "      <td>1.603135</td>\n",
       "      <td>0.263029</td>\n",
       "      <td>1.235263</td>\n",
       "      <td>-0.919418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             48        50        80        84        98        110       120  \\\n",
       "gre    -1.278605  0.452749  0.972155 -0.759199  0.972155  0.799020 -0.586063   \n",
       "gpa    -2.390908  1.235263 -1.287291  0.552071 -1.339844 -0.814312  0.919944   \n",
       "rank_1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "rank_2  0.000000  0.000000  0.000000  0.000000  1.000000  0.000000  1.000000   \n",
       "rank_3  0.000000  1.000000  0.000000  1.000000  0.000000  0.000000  0.000000   \n",
       "rank_4  1.000000  0.000000  1.000000  0.000000  0.000000  1.000000  0.000000   \n",
       "\n",
       "             122       133       148    ...          309       312       315  \\\n",
       "gre    -0.586063 -0.759199 -0.932334    ...    -1.278605  0.625884 -2.490553   \n",
       "gpa    -1.418674 -0.814312 -1.261014    ...    -1.077078  0.998773 -1.444950   \n",
       "rank_1  0.000000  0.000000  1.000000    ...     0.000000  0.000000  0.000000   \n",
       "rank_2  0.000000  0.000000  0.000000    ...     0.000000  0.000000  1.000000   \n",
       "rank_3  1.000000  1.000000  0.000000    ...     1.000000  1.000000  0.000000   \n",
       "rank_4  0.000000  0.000000  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "\n",
       "             317       328       356       368       375       386       396  \n",
       "gre     1.664697 -0.066657  0.972155 -0.066657 -0.239793  1.318426 -0.239793  \n",
       "gpa     0.630901 -0.131120 -0.315056  1.603135  0.263029  1.235263 -0.919418  \n",
       "rank_1  0.000000  0.000000  0.000000  1.000000  0.000000  0.000000  0.000000  \n",
       "rank_2  0.000000  1.000000  1.000000  0.000000  0.000000  1.000000  0.000000  \n",
       "rank_3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  1.000000  \n",
       "rank_4  1.000000  0.000000  0.000000  0.000000  1.000000  0.000000  0.000000  \n",
       "\n",
       "[6 rows x 40 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "admissions = pd.read_csv('datasets/binary.csv')\n",
    "\n",
    "# Make dummy variables for rank\n",
    "data = pd.concat([admissions, pd.get_dummies(admissions['rank'], prefix='rank')], axis=1)\n",
    "data = data.drop('rank', axis=1)\n",
    "\n",
    "# Standarize features\n",
    "for field in ['gre', 'gpa']:\n",
    "    mean, std = data[field].mean(), data[field].std()\n",
    "    data.loc[:,field] = (data[field]-mean)/std\n",
    "    \n",
    "# Split off random 10% of the data for testing\n",
    "np.random.seed(21)\n",
    "sample = np.random.choice(data.index, size=int(len(data)*0.9), replace=False)\n",
    "data, test_data = data.iloc[sample], data.drop(sample)\n",
    "\n",
    "# Split into features and targets\n",
    "features, targets = data.drop('admit', axis=1), data['admit']\n",
    "features_test, targets_test = test_data.drop('admit', axis=1), test_data['admit']\n",
    "features_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(21)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "n_hidden = 2  # number of hidden units\n",
    "epochs = 900\n",
    "learnrate = 0.005\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "# Initialize weights\n",
    "weights_input_hidden = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                        size=(n_features, n_hidden))\n",
    "weights_hidden_output = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                         size=n_hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: 0.5707935369711756\n",
      "error: 0.42920646302882437\n",
      "output_error_term: 0.1051505510491229\n",
      "hidden_error: [ 0.06406563 -0.03500296]\n",
      "hidden_error_term: [ 0.01513006 -0.00696304]\n",
      "del_w_hidden_output: [ 0.06494334  0.02881197]\n",
      "x: [ 0.97215519  0.44696493  1.          0.          0.          0.        ]\n",
      "del_w_input_hidden: [[ 0.01470876 -0.00676915]\n",
      " [ 0.0067626  -0.00311223]\n",
      " [ 0.01513006 -0.00696304]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]]\n",
      "---\n",
      "Prediction accuracy: 0.500\n"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    del_w_input_hidden = np.zeros(weights_input_hidden.shape)\n",
    "    del_w_hidden_output = np.zeros(weights_hidden_output.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        ## Forward pass ##\n",
    "        # TODO: Calculate the output\n",
    "        hidden_input = np.dot(x, weights_input_hidden)\n",
    "        hidden_output = sigmoid(hidden_input)\n",
    "        output = sigmoid(np.dot(hidden_output, weights_hidden_output))\n",
    "        print(\"output: {}\".format(output))\n",
    "        \n",
    "        ## Backward pass ##\n",
    "        # TODO: Calculate the network's prediction error\n",
    "        error = y - output\n",
    "        print(\"error: {}\".format(error))\n",
    "\n",
    "        # TODO: Calculate error term for the output unit\n",
    "        output_error_term = error * output * (1 - output)\n",
    "        print(\"output_error_term: {}\".format(output_error_term))\n",
    "\n",
    "        ## propagate errors to hidden layer\n",
    "\n",
    "        # TODO: Calculate the hidden layer's contribution to the error\n",
    "        hidden_error = np.dot(output_error_term, weights_hidden_output)\n",
    "        print(\"hidden_error: {}\".format(hidden_error))\n",
    "\n",
    "        # TODO: Calculate the error term for the hidden layer\n",
    "        hidden_error_term = hidden_error * hidden_output * (1 - hidden_output)\n",
    "        print(\"hidden_error_term: {}\".format(hidden_error_term))\n",
    "        \n",
    "        # TODO: Update the change in weights\n",
    "        del_w_hidden_output += output_error_term * hidden_output\n",
    "        print(\"del_w_hidden_output: {}\".format(del_w_hidden_output))\n",
    "        print(\"x: {}\".format(x))\n",
    "        del_w_input_hidden += hidden_error_term * x[:, None]\n",
    "        print(\"del_w_input_hidden: {}\".format(del_w_input_hidden))\n",
    "        print(\"---\")\n",
    "        break\n",
    "\n",
    "    # TODO: Update weights\n",
    "    weights_input_hidden += learnrate * del_w_input_hidden / n_records\n",
    "    weights_hidden_output += learnrate * del_w_hidden_output / n_records\n",
    "    break\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        hidden_output = sigmoid(np.dot(x, weights_input_hidden))\n",
    "        out = sigmoid(np.dot(hidden_output,\n",
    "                             weights_hidden_output))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "hidden = sigmoid(np.dot(features_test, weights_input_hidden))\n",
    "out = sigmoid(np.dot(hidden, weights_hidden_output))\n",
    "predictions = out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
